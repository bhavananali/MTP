{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dbe7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from seamless_communication.inference import Translator\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "from jiwer import wer\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_v2. Set `force` to `True` to download again.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Setup\n",
    "# -------------------------------\n",
    "model_name = \"seamlessM4T_v2_large\"\n",
    "vocoder_name = \"vocoder_v2\" if model_name == \"seamlessM4T_v2_large\" else \"vocoder_36langs\"\n",
    "\n",
    "translator = Translator(\n",
    "    model_name,\n",
    "    vocoder_name,\n",
    "    device=torch.device(\"cuda\"),\n",
    "    dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cde11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Language mapping: SeamlessM4T â†’ FLEURS\n",
    "lang_map_full = {\n",
    "    \"tel\": \"te_in\",   # Telugu\n",
    "    \"urd\": \"ur_pk\",   # Urdu\n",
    "}\n",
    "lang_map_partial = {\n",
    "    \"tam\": \"ta_in\",   # Tamil\n",
    "    \"ory\": \"or_in\",   # Odia\n",
    "}\n",
    "\n",
    "fleurs_src_lang = \"hi_in\"   # Hindi (FLEURS)\n",
    "sm4t_src_lang = \"hin\"       # Hindi (SM4T)\n",
    "\n",
    "# Output folder\n",
    "OUTPUT_DIR = \"./translation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5715e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_16k(audio, orig_sr):\n",
    "    \"\"\"Resample audio to 16kHz.\"\"\"\n",
    "    return torchaudio.transforms.Resample(orig_sr, 16000)(torch.tensor(audio)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7147e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, lang_code):\n",
    "    \"\"\"Save DataFrame to CSV.\"\"\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{lang_code}_results.csv\")\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"ðŸ’¾ Saved {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb5902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_translation_for_language(sm4t_tgt_lang, fleurs_tgt_lang, full_tasks=True):\n",
    "    \"\"\"\n",
    "    Run translation pipeline for one language.\n",
    "    full_tasks=True â†’ Run all 4 tasks\n",
    "    full_tasks=False â†’ Run only S2TT, T2TT\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸ”¹ Processing Target Language: {sm4t_tgt_lang.upper()} ({fleurs_tgt_lang})\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load datasets\n",
    "    src_dataset = load_dataset(\"google/fleurs\", fleurs_src_lang, split=\"test\")\n",
    "    tgt_dataset = load_dataset(\"google/fleurs\", fleurs_tgt_lang, split=\"test\")\n",
    "\n",
    "    src_by_id = {item[\"id\"]: item for item in src_dataset}\n",
    "    tgt_by_id = {item[\"id\"]: item for item in tgt_dataset}\n",
    "    common_ids = sorted(set(src_by_id.keys()) & set(tgt_by_id.keys()))\n",
    "\n",
    "    print(f\"Found {len(common_ids)} parallel sentences\")\n",
    "\n",
    "    references, hypotheses_s2tt, hypotheses_t2tt = [], [], []\n",
    "    predicted_s2s, predicted_t2s = [], []\n",
    "    source_texts = []\n",
    "\n",
    "    for sentence_id in common_ids:\n",
    "        src = src_by_id[sentence_id]\n",
    "        tgt = tgt_by_id[sentence_id]\n",
    "\n",
    "        src_audio = src[\"audio\"][\"array\"]\n",
    "        src_sr = src[\"audio\"][\"sampling_rate\"]\n",
    "        src_text = src[\"transcription\"]\n",
    "        tgt_text = tgt[\"transcription\"]\n",
    "\n",
    "        references.append([tgt_text])\n",
    "        source_texts.append(src_text)\n",
    "\n",
    "        if src_sr != 16000:\n",
    "            src_audio = resample_to_16k(src_audio, src_sr)\n",
    "        base_dir = \"/scratch/aj/Bhavna/bhav_venv_311/Project/input_audios\"\n",
    "\n",
    "            # Create language-specific folder\n",
    "        lang_dir = os.path.join(base_dir, sm4t_tgt_lang)\n",
    "        os.makedirs(lang_dir, exist_ok=True)\n",
    "\n",
    "        audio_path = os.path.join(lang_dir, f\"input_{sm4t_tgt_lang}_{sentence_id}.wav\")\n",
    "        #audio_path = f\"/scratch/aj/Bhavna/bhav_venv_311/Project/input_audios/input{sm4t_tgt_lang}_{sentence_id}.wav\"\n",
    "        torchaudio.save(audio_path, torch.tensor(src_audio).unsqueeze(0), 16000)\n",
    "\n",
    "        # --- S2TT ---\n",
    "        s2tt_out, _ = translator.predict(\n",
    "            input=audio_path, task_str=\"s2tt\", tgt_lang=sm4t_tgt_lang\n",
    "        )\n",
    "        hypotheses_s2tt.append(s2tt_out[0])\n",
    "\n",
    "        # --- T2TT ---\n",
    "        t2tt_out, _ = translator.predict(\n",
    "            input=src_text, task_str=\"t2tt\", src_lang=sm4t_src_lang, tgt_lang=sm4t_tgt_lang\n",
    "        )\n",
    "        hypotheses_t2tt.append(t2tt_out[0])\n",
    "\n",
    "        if full_tasks:\n",
    "            # --- S2ST + ASR ---\n",
    "            _, s2s_audio_out = translator.predict(\n",
    "                input=audio_path, task_str=\"s2st\", tgt_lang=sm4t_tgt_lang\n",
    "            )\n",
    "            # Define base directory\n",
    "            base_dir = \"/scratch/aj/Bhavna/bhav_venv_311/Project/s2s_outputs\"\n",
    "\n",
    "            # Create language-specific folder\n",
    "            lang_dir = os.path.join(base_dir, sm4t_tgt_lang)\n",
    "            os.makedirs(lang_dir, exist_ok=True)\n",
    "\n",
    "            # Final file path\n",
    "            s2s_path = os.path.join(lang_dir, f\"s2s_{sm4t_tgt_lang}_{sentence_id}.wav\")\n",
    "\n",
    "            #s2s_path = f\"/scratch/aj/Bhavna/bhav_venv_311/Project/s2s_outputs/s2s{sm4t_tgt_lang}_{sentence_id}.wav\"\n",
    "            torchaudio.save(\n",
    "                s2s_path,\n",
    "                s2s_audio_out.audio_wavs[0][0].to(torch.float32).cpu(),\n",
    "                s2s_audio_out.sample_rate,\n",
    "            )\n",
    "            s2s_asr_out, _ = translator.predict(\n",
    "                input=s2s_path, task_str=\"asr\", tgt_lang=sm4t_tgt_lang\n",
    "            )\n",
    "            predicted_s2s.append(s2s_asr_out[0])\n",
    "\n",
    "            # --- T2ST + ASR ---\n",
    "            _, t2s_audio_out = translator.predict(\n",
    "                input=src_text, task_str=\"t2st\", src_lang=sm4t_src_lang, tgt_lang=sm4t_tgt_lang\n",
    "            )\n",
    "            # Define base directory\n",
    "            base_dir = \"/scratch/aj/Bhavna/bhav_venv_311/Project/t2s_outputs\"\n",
    "\n",
    "            # Create language-specific folder\n",
    "            lang_dir = os.path.join(base_dir, sm4t_tgt_lang)\n",
    "            os.makedirs(lang_dir, exist_ok=True)\n",
    "\n",
    "            # Final file path\n",
    "            t2s_path = os.path.join(lang_dir, f\"t2s_{sm4t_tgt_lang}_{sentence_id}.wav\")\n",
    "\n",
    "            #t2s_path = f\"/scratch/aj/Bhavna/bhav_venv_311/Project/t2s_outputs/t2s{sm4t_tgt_lang}_{sentence_id}.wav\"\n",
    "            torchaudio.save(\n",
    "                t2s_path,\n",
    "                t2s_audio_out.audio_wavs[0][0].to(torch.float32).cpu(),\n",
    "                t2s_audio_out.sample_rate,\n",
    "            )\n",
    "            t2s_asr_out, _ = translator.predict(\n",
    "                input=t2s_path, task_str=\"asr\", tgt_lang=sm4t_tgt_lang\n",
    "            )\n",
    "            predicted_t2s.append(t2s_asr_out[0])\n",
    "\n",
    "    # Build dataframe\n",
    "    data = {\n",
    "        \"source_text\": source_texts,\n",
    "        \"reference_text\": [r[0] for r in references],\n",
    "        \"S2TT_prediction\": hypotheses_s2tt,\n",
    "        \"T2TT_prediction\": hypotheses_t2tt,\n",
    "    }\n",
    "    if full_tasks:\n",
    "        data[\"S2ST_ASR\"] = predicted_s2s\n",
    "        data[\"T2ST_ASR\"] = predicted_t2s\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    save_dataframe(df, sm4t_tgt_lang)\n",
    "\n",
    "    return references, hypotheses_s2tt, hypotheses_t2tt, predicted_s2s, predicted_t2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df85d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "from jiwer import wer\n",
    "\n",
    "def compute_metrics(lang_code, references, hypotheses_s2tt, hypotheses_t2tt, predicted_s2s, predicted_t2s):\n",
    "    # Normalize hyps\n",
    "    hypotheses_s2tt = [\" \".join(h) if isinstance(h, list) else str(h) for h in hypotheses_s2tt]\n",
    "    hypotheses_t2tt = [\" \".join(h) if isinstance(h, list) else str(h) for h in hypotheses_t2tt]\n",
    "    predicted_s2s   = [\" \".join(h) if isinstance(h, list) else str(h) for h in predicted_s2s]\n",
    "    predicted_t2s   = [\" \".join(h) if isinstance(h, list) else str(h) for h in predicted_t2s]\n",
    "\n",
    "    # Normalize refs\n",
    "    references_norm = []\n",
    "    for refset in references:\n",
    "        references_norm.append([\" \".join(r) if isinstance(r, list) else str(r) for r in refset])\n",
    "    multi_references = list(zip(*references_norm))  # multiple references per sentence\n",
    "\n",
    "    metrics = {}\n",
    "    # Text-to-text & speech-to-text\n",
    "    metrics[\"S2TT_SacreBLEU\"] = corpus_bleu(hypotheses_s2tt, multi_references).score\n",
    "    metrics[\"T2TT_chrF2++\"]   = corpus_chrf(hypotheses_t2tt, multi_references).score\n",
    "    metrics[\"S2TT_WER\"]       = sum(wer(ref[0], hyp) for ref, hyp in zip(references_norm, hypotheses_s2tt)) / len(references_norm)\n",
    "    metrics[\"T2TT_WER\"]       = sum(wer(ref[0], hyp) for ref, hyp in zip(references_norm, hypotheses_t2tt)) / len(references_norm)\n",
    "\n",
    "    # Speech-to-speech (decoded to text for scoring)\n",
    "    if predicted_s2s:\n",
    "        metrics[\"S2ST_ASR_WER\"]  = sum(wer(ref[0], hyp) for ref, hyp in zip(references_norm, predicted_s2s)) / len(references_norm)\n",
    "        metrics[\"S2ST_ASR_BLEU\"] = corpus_bleu(predicted_s2s, multi_references).score\n",
    "\n",
    "    if predicted_t2s:\n",
    "        metrics[\"T2ST_ASR_WER\"]  = sum(wer(ref[0], hyp) for ref, hyp in zip(references_norm, predicted_t2s)) / len(references_norm)\n",
    "        metrics[\"T2ST_ASR_BLEU\"] = corpus_bleu(predicted_t2s, multi_references).score\n",
    "        \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950c70a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ”¹ Processing Target Language: TEL (te_in)\n",
      "============================================================\n",
      "Found 228 parallel sentences\n",
      "ðŸ’¾ Saved ./translation_results/tel_results.csv\n",
      "ðŸ”¹ TEL Metrics: {'S2TT_SacreBLEU': 6.791687079616478, 'T2TT_chrF2++': 49.22213115874377, 'S2TT_WER': 0.9214235330398954, 'T2TT_WER': 0.85229959325962, 'S2ST_ASR_WER': 0.9207014212978243, 'S2ST_ASR_BLEU': 6.703186032040358, 'T2ST_ASR_WER': 0.8816193817330296, 'T2ST_ASR_BLEU': 9.072877840222192}\n",
      "\n",
      "============================================================\n",
      "ðŸ”¹ Processing Target Language: URD (ur_pk)\n",
      "============================================================\n",
      "Found 176 parallel sentences\n",
      "ðŸ’¾ Saved ./translation_results/urd_results.csv\n",
      "ðŸ”¹ URD Metrics: {'S2TT_SacreBLEU': 14.627013884604041, 'T2TT_chrF2++': 44.07527441374375, 'S2TT_WER': 0.7461084350137203, 'T2TT_WER': 0.7154703562371476, 'S2ST_ASR_WER': 0.7457257921461948, 'S2ST_ASR_BLEU': 14.802831740006779, 'T2ST_ASR_WER': 0.7231635694724213, 'T2ST_ASR_BLEU': 16.53967566437534}\n",
      "\n",
      "============================================================\n",
      "ðŸ”¹ Processing Target Language: TAM (ta_in)\n",
      "============================================================\n",
      "Found 253 parallel sentences\n",
      "ðŸ’¾ Saved ./translation_results/tam_results.csv\n",
      "ðŸ”¹ TAM Metrics: {'S2TT_SacreBLEU': 4.273384897001094, 'T2TT_chrF2++': 51.03895076901964, 'S2TT_WER': 0.9191945256006866, 'T2TT_WER': 0.8604079345279637}\n",
      "\n",
      "============================================================\n",
      "ðŸ”¹ Processing Target Language: ORY (or_in)\n",
      "============================================================\n",
      "Found 251 parallel sentences\n",
      "ðŸ’¾ Saved ./translation_results/ory_results.csv\n",
      "ðŸ”¹ ORY Metrics: {'S2TT_SacreBLEU': 5.73832384752016, 'T2TT_chrF2++': 45.167168574181915, 'S2TT_WER': 0.8712041110972176, 'T2TT_WER': 0.8172689715610361}\n",
      "\n",
      "âœ… All processing complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main Driver\n",
    "# -------------------------------\n",
    "all_metrics = {}\n",
    "\n",
    "# Full tasks: Telugu + Urdu\n",
    "for sm4t_tgt, fleurs_tgt in lang_map_full.items():\n",
    "    refs, s2tt, t2tt, s2s, t2s = run_translation_for_language(sm4t_tgt, fleurs_tgt, full_tasks=True)\n",
    "    all_metrics[sm4t_tgt] = compute_metrics(sm4t_tgt, refs, s2tt, t2tt, s2s, t2s)\n",
    "    print(f\"ðŸ”¹ {sm4t_tgt.upper()} Metrics: {all_metrics[sm4t_tgt]}\")\n",
    "# Partial tasks: Tamil + Odia\n",
    "for sm4t_tgt, fleurs_tgt in lang_map_partial.items():\n",
    "    refs, s2tt, t2tt, _, _ = run_translation_for_language(sm4t_tgt, fleurs_tgt, full_tasks=False)\n",
    "    all_metrics[sm4t_tgt] = compute_metrics(sm4t_tgt, refs, s2tt, t2tt, [], [])\n",
    "    print(f\"ðŸ”¹ {sm4t_tgt.upper()} Metrics: {all_metrics[sm4t_tgt]}\")\n",
    "\n",
    "print(\"\\nâœ… All processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhav_venv_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
